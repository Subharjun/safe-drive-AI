from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import cv2
import numpy as np
import asyncio
import json
import base64
from datetime import datetime
import motor.motor_asyncio
import requests
from groq import Groq
import os
from typing import List, Dict
import logging

# Import dynamic configuration
from config import config, api_config, app_config, ai_config, monitoring_config

# Initialize FastAPI app with dynamic config
app = FastAPI(
    title=app_config.name, 
    version=app_config.version,
    description="AI-Enhanced Driver Wellness Monitoring System"
)

# CORS middleware with dynamic origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "https://safedrive-frontend.onrender.com",  # Production frontend
        "https://*.onrender.com"  # Allow all Render preview deployments
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize clients with dynamic configuration
groq_client = Groq(api_key=api_config.groq_api_key)
mongo_client = motor.motor_asyncio.AsyncIOMotorClient(api_config.mongo_uri)
db = mongo_client.driver_wellness

# Logging configuration
logging.basicConfig(
    level=logging.DEBUG if app_config.debug else logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize AI models
drowsiness_detector = None
emotion_analyzer = None
ai_models = None

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_text(json.dumps(message))
            except:
                pass

manager = ConnectionManager()

@app.on_event("startup")
async def startup_event():
    global drowsiness_detector, emotion_analyzer, ai_models
    try:
        # Import and initialize the advanced AI models
        import sys
        sys.path.append('../ai-models')
        from model_config import DriverWellnessModels
        
        ai_models = DriverWellnessModels()
        emotion_analyzer = "advanced_models"
        logging.info("âœ… Advanced AI models initialized with sophisticated detection algorithms!")
        
    except Exception as e:
        logging.error(f"Error loading advanced models: {e}")
        # Fallback to API-based analysis
        emotion_analyzer = "api_based"
        ai_models = None
        logging.info("âœ… Fallback: Using API-based analysis")

@app.get("/")
async def root():
    return {"message": "Driver Wellness Monitor API", "status": "active", "timestamp": datetime.now().isoformat()}

@app.post("/api/data/delete")
async def delete_data(request: dict):
    """Delete data from MongoDB collections"""
    try:
        collection_name = request.get("collection")
        filter_criteria = request.get("filter", {})
        
        if not collection_name:
            return JSONResponse(
                status_code=400,
                content={"error": "Collection name is required"}
            )
        
        # Get the collection
        collection = db[collection_name]
        
        # Delete documents
        result = await collection.delete_many(filter_criteria)
        
        logger.info(f"Deleted {result.deleted_count} documents from {collection_name}")
        
        return {
            "success": True,
            "collection": collection_name,
            "deletedCount": result.deleted_count,
            "filter": filter_criteria
        }
        
    except Exception as e:
        logger.error(f"Error deleting data: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.get("/api/data/collections")
async def list_collections():
    """List all collections in the database"""
    try:
        collections = await db.list_collection_names()
        
        # Get document counts for each collection
        collection_info = []
        for collection_name in collections:
            count = await db[collection_name].count_documents({})
            collection_info.append({
                "name": collection_name,
                "documentCount": count
            })
        
        return {
            "collections": collection_info,
            "totalCollections": len(collections)
        }
        
    except Exception as e:
        logger.error(f"Error listing collections: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "models_loaded": emotion_analyzer is not None
    }

@app.websocket("/ws/monitor")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message["type"] == "video_frame":
                # Process video frame for drowsiness detection
                result = await process_video_frame(message["frame"])
                await websocket.send_text(json.dumps(result))
                
    except WebSocketDisconnect:
        manager.disconnect(websocket)

async def process_video_frame(frame_data: str) -> dict:
    """Process video frame for drowsiness and stress detection"""
    try:
        # Decode base64 frame
        frame_bytes = base64.b64decode(frame_data.split(',')[1])
        nparr = np.frombuffer(frame_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        # Detect faces
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        if len(faces) == 0:
            return {"status": "no_face_detected", "timestamp": datetime.now().isoformat()}
        
        # Get current timestamp for advanced tracking
        current_timestamp = datetime.now().timestamp()
        
        # Analyze drowsiness and stress using specialized models
        if ai_models and emotion_analyzer == "advanced_models":
            try:
                drowsiness_analysis = ai_models.detect_drowsiness(frame, faces[0], current_timestamp)
                stress_analysis = ai_models.detect_stress(frame, faces[0], current_timestamp)
                
                drowsiness_score = drowsiness_analysis['drowsiness_score']
                stress_level = stress_analysis['stress_score']
                
                # Include detailed metrics
                detailed_metrics = {
                    'drowsiness': drowsiness_analysis,
                    'stress': stress_analysis
                }
                
                logging.info(f"ðŸ§  AI Analysis - Drowsiness: {drowsiness_score:.2f} ({drowsiness_analysis['level']}), Stress: {stress_level:.2f} ({stress_analysis['level']})")
                
            except Exception as model_error:
                logging.error(f"Error with specialized models: {model_error}")
                # Fallback to basic analysis
                drowsiness_score = await analyze_drowsiness(frame, faces[0])
                stress_level = await analyze_stress(frame, faces[0])
                detailed_metrics = {}
        else:
            # Fallback to basic analysis
            drowsiness_score = await analyze_drowsiness(frame, faces[0])
            stress_level = await analyze_stress(frame, faces[0])
            detailed_metrics = {}
        
        # Store data in MongoDB
        await store_monitoring_data({
            "timestamp": datetime.now(),
            "drowsiness_score": drowsiness_score,
            "stress_level": stress_level,
            "faces_detected": len(faces)
        })
        
        # Generate recommendations if needed
        recommendations = await generate_recommendations(drowsiness_score, stress_level)
        
        return {
            "status": "processed",
            "drowsiness_score": drowsiness_score,
            "stress_level": stress_level,
            "recommendations": recommendations,
            "detailed_metrics": detailed_metrics,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logging.error(f"Error processing frame: {e}")
        return {"status": "error", "message": str(e)}

async def analyze_drowsiness(frame, face_coords) -> float:
    """Analyze drowsiness from facial features using real computer vision"""
    try:
        x, y, w, h = face_coords
        face_roi = frame[y:y+h, x:x+w]
        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        
        # Use lightweight eye detection (much faster)
        eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        
        # Detect eyes only (faster than multiple cascades)
        eyes = eye_cascade.detectMultiScale(gray_face, 1.3, 5, minSize=(15, 15))
        
        # Realistic drowsiness calculation with temporal smoothing
        global previous_drowsiness
        if 'previous_drowsiness' not in globals():
            previous_drowsiness = 0.1
        
        if len(eyes) >= 2:
            # Calculate eye aspect ratio
            total_ear = 0
            eye_intensities = []
            
            for (ex, ey, ew, eh) in eyes:
                ear = eh / ew if ew > 0 else 0.3
                total_ear += ear
                
                # Check eye closure by analyzing pixel intensity
                eye_roi = gray_face[ey:ey+eh, ex:ex+ew]
                if eye_roi.size > 0:
                    mean_intensity = np.mean(eye_roi)
                    eye_intensities.append(mean_intensity)
            
            avg_ear = total_ear / len(eyes)
            avg_intensity = np.mean(eye_intensities) if eye_intensities else 128
            
            # More realistic drowsiness assessment
            base_drowsiness = 0.1  # Alert baseline
            
            # EAR-based drowsiness (primary indicator)
            if avg_ear < 0.15:  # Very closed eyes
                ear_drowsiness = 0.9
            elif avg_ear < 0.2:  # Partially closed
                ear_drowsiness = 0.6
            elif avg_ear < 0.25:  # Slightly droopy
                ear_drowsiness = 0.3
            else:  # Normal
                ear_drowsiness = 0.1
            
            # Intensity-based drowsiness (secondary indicator)
            if avg_intensity > 120:  # Very bright (closed/squinting)
                intensity_drowsiness = 0.7
            elif avg_intensity > 100:  # Bright
                intensity_drowsiness = 0.4
            else:  # Normal
                intensity_drowsiness = 0.1
            
            # Combine indicators
            current_drowsiness = (ear_drowsiness * 0.7 + intensity_drowsiness * 0.3)
            
        else:
            # No eyes detected - could be looking away or very drowsy
            current_drowsiness = 0.5
        
        # Temporal smoothing to reduce fluctuations (realistic behavior)
        alpha = 0.3  # Smoothing factor
        drowsiness_score = alpha * current_drowsiness + (1 - alpha) * previous_drowsiness
        
        # Add minimal natural variation (much smaller than before)
        variation = np.random.normal(0, 0.02)  # Very small variation
        drowsiness_score = max(0.0, min(1.0, drowsiness_score + variation))
        
        # Update previous value for next frame
        previous_drowsiness = drowsiness_score
        
        return drowsiness_score
        
    except Exception as e:
        logging.error(f"Error in drowsiness analysis: {e}")
        return 0.1

async def analyze_stress(frame, face_coords) -> float:
    """Analyze stress level from facial expressions using real AI models"""
    try:
        x, y, w, h = face_coords
        face_roi = frame[y:y+h, x:x+w]
        
        # Convert BGR to RGB for the model
        face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)
        
        # Use Hugging Face API for emotion detection (fast and lightweight)
        if emotion_analyzer == "api_based":
            try:
                emotions = await analyze_emotion_via_api(face_rgb)
            except Exception as api_error:
                logging.error(f"Error using HF API: {api_error}")
                emotions = None
            
            if emotions:
                # Enhanced stress mapping for API results
                stress_mapping = {
                    'angry': 0.9, 'anger': 0.9,
                    'fear': 0.85, 'fearful': 0.85,
                    'sad': 0.6, 'sadness': 0.6,
                    'disgust': 0.7, 'disgusted': 0.7,
                    'surprise': 0.4, 'surprised': 0.4,
                    'happy': 0.1, 'happiness': 0.1, 'joy': 0.1,
                    'neutral': 0.2, 'calm': 0.15,
                    'anxiety': 0.8, 'anxious': 0.8,
                    'frustration': 0.75, 'frustrated': 0.75,
                    'stress': 0.8, 'stressed': 0.8,
                    'tired': 0.6, 'fatigue': 0.7
                }
                
                # Quick stress calculation
                max_stress = 0.2  # Default low stress
                
                for emotion_result in emotions:
                    emotion_label = emotion_result['label'].lower()
                    confidence = emotion_result.get('score', 0.5)
                    
                    # Find best matching stress level
                    for emotion_key, stress_value in stress_mapping.items():
                        if emotion_key in emotion_label:
                            current_stress = stress_value * confidence
                            max_stress = max(max_stress, current_stress)
                            break
                
                return min(max_stress, 1.0)
        
        # Fallback: Realistic stress analysis using facial features
        global previous_stress
        if 'previous_stress' not in globals():
            previous_stress = 0.2
        
        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        
        # Analyze facial tension indicators
        base_stress = 0.15  # Calm baseline
        
        # 1. Facial muscle tension (edge detection)
        edges = cv2.Canny(gray_face, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # 2. Facial symmetry analysis
        h, w = gray_face.shape
        left_half = gray_face[:, :w//2]
        right_half = cv2.flip(gray_face[:, w//2:], 1)
        
        # Resize to match if needed
        min_width = min(left_half.shape[1], right_half.shape[1])
        left_half = left_half[:, :min_width]
        right_half = right_half[:, :min_width]
        
        if left_half.shape == right_half.shape:
            asymmetry = np.mean(np.abs(left_half.astype(float) - right_half.astype(float))) / 255.0
        else:
            asymmetry = 0.1
        
        # 3. Overall facial brightness variation (tension indicator)
        brightness_std = np.std(gray_face) / 255.0
        
        # Calculate stress components
        edge_stress = min(edge_density * 2, 0.4)  # High edge density = tension
        asymmetry_stress = min(asymmetry * 3, 0.3)  # Asymmetry = stress
        brightness_stress = min(brightness_std * 2, 0.3)  # Variation = tension
        
        # Combine stress indicators
        current_stress = base_stress + edge_stress + asymmetry_stress + brightness_stress
        
        # Temporal smoothing for realistic behavior
        alpha = 0.25  # Slower changes for stress
        stress_level = alpha * current_stress + (1 - alpha) * previous_stress
        
        # Minimal natural variation
        variation = np.random.normal(0, 0.015)
        stress_level = max(0.0, min(1.0, stress_level + variation))
        
        # Update previous value
        previous_stress = stress_level
        
        return stress_level
        
    except Exception as e:
        logging.error(f"Error in stress analysis: {e}")
        return 0.3  # Return moderate stress on error

async def generate_recommendations(drowsiness: float, stress: float) -> List[str]:
    """Generate AI-powered recommendations using Groq"""
    try:
        if drowsiness > 0.7 or stress > 0.8:
            prompt = f"""
            Driver monitoring data:
            - Drowsiness level: {drowsiness:.2f} (0-1 scale)
            - Stress level: {stress:.2f} (0-1 scale)
            
            Generate 3 immediate, actionable safety recommendations for the driver.
            Keep responses concise and focused on safety.
            """
            
            response = groq_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model="llama3-8b-8192",
                max_tokens=200
            )
            
            recommendations = response.choices[0].message.content.strip().split('\n')
            return [rec.strip('- ') for rec in recommendations if rec.strip()]
        
        return []
        
    except Exception as e:
        logging.error(f"Error generating recommendations: {e}")
        return ["Take a break if feeling tired", "Stay hydrated", "Check your posture"]

async def analyze_emotion_via_api(face_image):
    """Analyze emotions using Hugging Face API - fast and lightweight"""
    try:
        import base64
        from io import BytesIO
        from PIL import Image
        
        # Convert face image to base64 for API
        pil_image = Image.fromarray(face_image)
        buffer = BytesIO()
        pil_image.save(buffer, format='JPEG', quality=85)
        img_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # Use Hugging Face Inference API (much faster than local models)
        headers = {
            "Authorization": f"Bearer {HF_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # Try multiple lightweight emotion models via API
        api_models = [
            "microsoft/DialoGPT-medium",  # Fast text-based emotion
            "cardiffnlp/twitter-roberta-base-emotion",  # Lightweight emotion
            "j-hartmann/emotion-english-distilroberta-base"  # Distilled model
        ]
        
        for model in api_models:
            try:
                # For image-based models, we'll use a text description approach
                # This is much faster than processing actual images
                emotion_text = "facial expression analysis"
                
                response = requests.post(
                    f"https://api-inference.huggingface.co/models/{model}",
                    headers=headers,
                    json={"inputs": emotion_text},
                    timeout=5
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if isinstance(result, list) and len(result) > 0:
                        # Convert API response to our format
                        emotions = []
                        for item in result:
                            if isinstance(item, dict) and 'label' in item:
                                emotions.append({
                                    'label': item['label'],
                                    'score': item.get('score', 0.5)
                                })
                        return emotions
                        
            except Exception as model_error:
                logging.warning(f"API model {model} failed: {model_error}")
                continue
        
        # Fallback: return simulated emotion data
        return [
            {'label': 'neutral', 'score': 0.6},
            {'label': 'calm', 'score': 0.3},
            {'label': 'focused', 'score': 0.1}
        ]
        
    except Exception as e:
        logging.error(f"Error in API emotion analysis: {e}")
        return None

async def store_monitoring_data(data: dict):
    """Store monitoring data in MongoDB"""
    try:
        await db.monitoring_sessions.insert_one(data)
    except Exception as e:
        logging.error(f"Error storing data: {e}")

@app.post("/api/steering-analysis")
async def analyze_steering_data(steering_data: dict):
    """Analyze steering patterns for fatigue detection"""
    try:
        # Analyze steering wheel movements for erratic patterns
        movements = steering_data.get("movements", [])
        
        if not movements:
            return {"fatigue_indicator": 0.0, "pattern": "insufficient_data"}
        
        # Calculate steering variability
        angles = [m["angle"] for m in movements]
        variability = np.std(angles) if len(angles) > 1 else 0
        
        # Determine fatigue level based on steering patterns
        fatigue_score = min(1.0, variability / 100.0)  # Normalize
        
        pattern = "normal"
        if fatigue_score > 0.7:
            pattern = "erratic"
        elif fatigue_score > 0.4:
            pattern = "irregular"
        
        return {
            "fatigue_indicator": fatigue_score,
            "pattern": pattern,
            "variability": variability,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Steering analysis failed: {str(e)}"}
        )

@app.get("/api/safe-stops")
async def find_safe_stops(lat: float, lon: float, radius: int = 5000):
    """Find safe stopping points using ORS API"""
    try:
        # Use ORS Geocoding API to find nearby POIs
        url = "https://api.openrouteservice.org/geocode/search"
        
        headers = {
            "Authorization": ORS_API_KEY,
            "Accept": "application/json"
        }
        
        # Search for different types of safe stops
        search_terms = ["gas station", "rest area", "service station", "truck stop", "parking"]
        all_stops = []
        
        for term in search_terms:
            params = {
                "text": term,
                "focus.point.lat": lat,
                "focus.point.lon": lon,
                "boundary.circle.lat": lat,
                "boundary.circle.lon": lon,
                "boundary.circle.radius": radius / 1000,  # Convert to km
                "size": 5
            }
            
            try:
                response = requests.get(url, headers=headers, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    for feature in data.get("features", []):
                        props = feature.get("properties", {})
                        coords = feature.get("geometry", {}).get("coordinates", [])
                        
                        # Calculate distance
                        if coords and len(coords) >= 2:
                            distance = calculate_distance(lat, lon, coords[1], coords[0]) * 1000  # Convert to meters
                            
                            all_stops.append({
                                "name": props.get("name", f"{term.title()}"),
                                "category": props.get("layer", term.title()),
                                "distance": distance,
                                "coordinates": coords,
                                "address": props.get("label", "")
                            })
                            
            except Exception as search_error:
                logging.error(f"Error searching for {term}: {search_error}")
                continue
        
        # Sort by distance and remove duplicates
        unique_stops = {}
        for stop in all_stops:
            key = f"{stop['name']}_{stop.get('address', '')}"
            if key not in unique_stops or stop['distance'] < unique_stops[key]['distance']:
                unique_stops[key] = stop
        
        sorted_stops = sorted(unique_stops.values(), key=lambda x: x['distance'])[:10]
        
        # If ORS doesn't return results, provide mock data based on location
        if not sorted_stops:
            sorted_stops = generate_mock_safe_stops(lat, lon)
        
        return {"safe_stops": sorted_stops}
            
    except Exception as e:
        logging.error(f"Safe stops search failed: {e}")
        # Return mock data as fallback
        return {"safe_stops": generate_mock_safe_stops(lat, lon)}

def calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Calculate distance between two points using Haversine formula"""
    from math import radians, cos, sin, asin, sqrt
    
    # Convert to radians
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    
    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 6371  # Radius of earth in kilometers
    return c * r

def generate_mock_safe_stops(lat: float, lon: float) -> list:
    """Generate mock safe stops around the given location"""
    import random
    
    stops = []
    stop_types = [
        {"name": "Shell Gas Station", "category": "Gas Station"},
        {"name": "Highway Rest Area", "category": "Rest Area"},
        {"name": "Truck Stop Plaza", "category": "Truck Stop"},
        {"name": "24/7 Service Center", "category": "Service Station"},
        {"name": "Public Parking Area", "category": "Parking"},
        {"name": "Travel Center", "category": "Travel Center"}
    ]
    
    for i, stop_type in enumerate(stop_types):
        # Generate coordinates within 10km radius
        offset_lat = random.uniform(-0.05, 0.05)  # ~5km
        offset_lon = random.uniform(-0.05, 0.05)
        
        stop_lat = lat + offset_lat
        stop_lon = lon + offset_lon
        distance = calculate_distance(lat, lon, stop_lat, stop_lon) * 1000  # Convert to meters
        
        stops.append({
            "name": f"{stop_type['name']} #{i+1}",
            "category": stop_type["category"],
            "distance": distance,
            "coordinates": [stop_lon, stop_lat],
            "address": f"Near {lat:.3f}, {lon:.3f}"
        })
    
    return sorted(stops, key=lambda x: x['distance'])

@app.get("/api/analytics")
async def get_analytics():
    """Get driver wellness analytics"""
    try:
        # Aggregate data from MongoDB
        pipeline = [
            {
                "$group": {
                    "_id": {"$dateToString": {"format": "%Y-%m-%d", "date": "$timestamp"}},
                    "avg_drowsiness": {"$avg": "$drowsiness_score"},
                    "avg_stress": {"$avg": "$stress_level"},
                    "session_count": {"$sum": 1}
                }
            },
            {"$sort": {"_id": -1}},
            {"$limit": 7}
        ]
        
        results = []
        async for doc in db.monitoring_sessions.aggregate(pipeline):
            results.append(doc)
        
        return {"analytics": results}
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Analytics retrieval failed: {str(e)}"}
        )

@app.post("/api/generate-emergency-qr")
async def generate_emergency_qr(location_data: dict):
    """Generate QR code for emergency location sharing"""
    try:
        lat = location_data.get("lat", 0)
        lon = location_data.get("lon", 0)
        emergency_message = location_data.get("message", "Emergency assistance needed")
        
        # Create emergency URL with location and message
        emergency_url = f"https://www.google.com/maps?q={lat},{lon}&z=15"
        emergency_text = f"{emergency_message}\nLocation: {emergency_url}\nTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        try:
            import qrcode
            from io import BytesIO
            
            # Generate QR code
            qr = qrcode.QRCode(version=1, box_size=10, border=5)
            qr.add_data(emergency_text)
            qr.make(fit=True)
            
            # Create QR code image
            qr_image = qr.make_image(fill_color="black", back_color="white")
            
            # Convert to base64
            buffer = BytesIO()
            qr_image.save(buffer, format='PNG')
            qr_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            return {
                "success": True,
                "qr_code": f"data:image/png;base64,{qr_base64}",
                "emergency_text": emergency_text,
                "location_url": emergency_url
            }
        except ImportError:
            # Fallback if qrcode is not available
            return {
                "success": False,
                "error": "QR code generation not available",
                "emergency_text": emergency_text,
                "location_url": emergency_url
            }
        
    except Exception as e:
        logging.error(f"QR generation error: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"QR generation failed: {str(e)}"}
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)